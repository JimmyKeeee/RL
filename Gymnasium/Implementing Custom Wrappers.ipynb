{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a6b59f-3d7f-456b-a83e-49ce6af504f8",
   "metadata": {},
   "source": [
    "# Inheriting from gymnasium.ObservationWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c79d414-3fff-43b9-87c8-5c9a80d88d38",
   "metadata": {},
   "source": [
    "Imagine you have a 2D navigation task where the environment returns dictionaries as observations with keys \"agent_position\" and \"target_position\". A common thing to do might be to throw away some degrees of freedom and only consider the position of the target relative to the agent, i.e. observation[\"target_position\"] - observation[\"agent_position\"]. For this, you could implement an observation wrapper like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3f14902-b664-4834-9eb3-92f94a4bdb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym import ActionWrapper, ObservationWrapper, RewardWrapper, Wrapper\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "\n",
    "\n",
    "class RelativePosition(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(shape=(2,), low=-np.inf, high=np.inf)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return obs[\"target\"] - obs[\"agent\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bd6a27-5152-460a-bf4c-de8751571ea4",
   "metadata": {},
   "source": [
    "# Inheriting from gymnasium.ActionWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2479d8d-cd78-4c15-b62a-ccfeec9bf48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteActions(ActionWrapper):\n",
    "    def __init__(self, env, disc_to_cont):\n",
    "        super().__init__(env)\n",
    "        self.disc_to_cont = disc_to_cont\n",
    "        self.action_space = Discrete(len(disc_to_cont))\n",
    "\n",
    "    def action(self, act):\n",
    "        return self.disc_to_cont[act]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "    wrapped_env = DiscreteActions(\n",
    "        env, [np.array([1, 0]), np.array([-1, 0]), np.array([0, 1]), np.array([0, -1])]\n",
    "    )\n",
    "    print(wrapped_env.action_space)  # Discrete(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cd18c3-c452-4d2b-8662-55b0460ebfc9",
   "metadata": {},
   "source": [
    "# Inheriting from gymnasium.RewardWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4361b88-e48f-4fe4-8343-4c7335170e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import SupportsFloat\n",
    "\n",
    "\n",
    "class ClipReward(RewardWrapper):\n",
    "    def __init__(self, env, min_reward, max_reward):\n",
    "        super().__init__(env)\n",
    "        self.min_reward = min_reward\n",
    "        self.max_reward = max_reward\n",
    "        self.reward_range = (min_reward, max_reward)\n",
    "\n",
    "    def reward(self, r: SupportsFloat) -> SupportsFloat:\n",
    "        return np.clip(r, self.min_reward, self.max_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac424601-deff-4d1b-8f76-30990f19d7ca",
   "metadata": {},
   "source": [
    "# Inheriting from gymnasium.Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d1cfc31-abd5-451d-b78b-2ac2d41a1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReacherRewardWrapper(Wrapper):\n",
    "    def __init__(self, env, reward_dist_weight, reward_ctrl_weight):\n",
    "        super().__init__(env)\n",
    "        self.reward_dist_weight = reward_dist_weight\n",
    "        self.reward_ctrl_weight = reward_ctrl_weight\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, _, terminated, truncated, info = self.env.step(action)\n",
    "        reward = (\n",
    "            self.reward_dist_weight * info[\"reward_dist\"]\n",
    "            + self.reward_ctrl_weight * info[\"reward_ctrl\"]\n",
    "        )\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0ff6674-ba84-46bd-9175-389fe45b4b84",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1852785005.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    git clone https://github.com/Farama-Foundation/gym-examples\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "git clone https://github.com/Farama-Foundation/gym-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60354088-ec2b-41af-a5f3-240e942189fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
