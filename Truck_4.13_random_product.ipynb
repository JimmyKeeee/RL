{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "# import pygame\n",
    "import torch\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.envs.registration import register\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for the action\n",
    "from pyclbr import Class\n",
    "\n",
    "\n",
    "class Action:\n",
    "    def __init__(self, name, delta):\n",
    "        self.name = name\n",
    "        self.delta = delta\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, index, coordinates, status):\n",
    "        self.index = index\n",
    "        self.coordinates = coordinates\n",
    "        self.status = status\n",
    "        self.terminate = False\n",
    "        self.truncated = False\n",
    "        self.reward = 0\n",
    "        self.volume = 0  # initialize volume to 0\n",
    "        self.sawlog_volume = 0  # initialize sawlog volume to 0\n",
    "        self.pulplog_volume = 0  # initialize pulplog volume to 0\n",
    "        \n",
    "    \n",
    "    def move(self, delta, grid_size):\n",
    "        # Calculate the new position\n",
    "        new_coordinates = tuple(sum(x) for x in zip(self.coordinates, delta))\n",
    "\n",
    "        # Check if the new position is within the grid\n",
    "        if 0 <= new_coordinates[0] < grid_size and 0 <= new_coordinates[1] < grid_size:\n",
    "            # If it is, move the agent to the new position\n",
    "            self.coordinates = new_coordinates\n",
    "        else:\n",
    "            # If it's not, move the agent to the edge of the grid\n",
    "            self.coordinates = (max(0, min(grid_size - 1, new_coordinates[0])),\n",
    "                                max(0, min(grid_size - 1, new_coordinates[1])))\n",
    "    def agent_load(self):\n",
    "        self.volume += 10  # increase volume by 10 when loading\n",
    "\n",
    "    \n",
    "    def logging_loading(self):\n",
    "        # create a random loading function, if the random number is greater than 0.25 (75% to load sawlog), the agent will load sawlog, otherwise, it will load pulplog\n",
    "        if random.random() > 0.75:\n",
    "            self.sawlog_volume += 10\n",
    "            self.volume += 10\n",
    "        else:\n",
    "            self.pulplog_volume += 10\n",
    "            self.volume += 10\n",
    "    \n",
    "    def agent_unload(self):\n",
    "        self.volume -= 10 # decrease volume by 10 when unloading\n",
    "\n",
    "    def update_status(self, new_status):\n",
    "        self.status = new_status\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Agent(location={self.location}, stats={self.status})\"\n",
    "\n",
    "# create a class of location - 3 types of location, log site, sort site, and mill, each location has a reward\n",
    "\n",
    "\n",
    "class Location:\n",
    "    def __init__(self, location_type, reward, coordinates, location_id):\n",
    "        self.location_type = location_type\n",
    "        self.reward = reward\n",
    "        self.coordinates = coordinates  # tuple of (x, y)\n",
    "        self.location_id = location_id  # unique numerical identifier\n",
    "        self.volume = 0  # initialize volume to 0\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Location(location_type={self.location_type}, reward={self.reward}, coordinates={self.coordinates}, location_id={self.location_id})\"\n",
    "    \n",
    "    def logging(self):\n",
    "        self.volume += 10  # increase volume by 10 when logging\n",
    "\n",
    "    def site_loading(self):\n",
    "        self.volume += 10  # increase volume by 10 when loading\n",
    "\n",
    "    def site_unloading(self):\n",
    "        self.volume -= 10  # decrease volume by 10 when unloading\n",
    "\n",
    "# create a class for the status, if the is status 0, it's just started, if the truck in status 1, it's loaded from log site, if the truck in status 2, it loaded sorted logs\n",
    "# from the sort site and unloaded raw logs to the sort site, if the truck in status 3, it's unloaded to the mill\n",
    "class Status():\n",
    "    STARTED = 0\n",
    "    LOADED_FROM_LOG_SITE = 1\n",
    "    LOADED_SORTED_LOGS = 2\n",
    "    UNLOADED_TO_MILL = 3\n",
    "\n",
    "    def __str__(self):\n",
    "        # Convert status to a more readable string.\n",
    "        if self == Status.STARTED:\n",
    "            return \"Just started\"\n",
    "        elif self == Status.LOADED_FROM_LOG_SITE:\n",
    "            return \"Loaded from log site\"\n",
    "        elif self == Status.LOADED_SORTED_LOGS:\n",
    "            return \"Loaded sorted logs from sort site and unloaded raw logs\"\n",
    "        elif self == Status.UNLOADED_TO_MILL:\n",
    "            return \"Unloaded to mill\"\n",
    "        else:\n",
    "            return \"Unknown status\"\n",
    "\n",
    "\n",
    "\n",
    "# create a class for the environment\n",
    "class ForestTruckEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode=\"human\", size = 8, reward =60, num_agents=4):\n",
    "        super(ForestTruckEnv, self).__init__()\n",
    "        self.reward = reward\n",
    "        self.num_agents = num_agents\n",
    "        self.size = size  # The size of the square grid\n",
    "        \n",
    "        self.render_mode = render_mode\n",
    "        self.done = False\n",
    "        self.truncated = False\n",
    "        self.t = 0\n",
    "        \n",
    "        # set up the random agent starting location\n",
    "        self.agents = [Agent(index, (random.randint(0, self.size-1), random.randint(0, self.size-1)), 0) for index in range(self.num_agents)]\n",
    "        self.available_actions = [\n",
    "            Action('up', (0, 1)),\n",
    "            Action('down', (0, -1)),\n",
    "            Action('left', (-1, 0)),\n",
    "            Action('right', (1, 0)),\n",
    "        ]\n",
    "        # set up the log sites, sort sites, and mills\n",
    "        locations = [(type, (x, y)) for type, coords in [('log_site', [(0, 0), (0, 3), (0, 7)]), \n",
    "                                                 ('sort_site', [(2, 2), (5, 2)]), \n",
    "                                                 ('sawmill', [(7, 7)]), \n",
    "                                                 ('pulpmill', [(7, 0)])] \n",
    "             for x, y in coords]\n",
    "\n",
    "        self.log_sites, self.sort_sites, self.sawmill, self.pulpmill = [], [], [], []\n",
    "\n",
    "        for i, (type, coord) in enumerate(locations):\n",
    "            location = Location(type, reward, coord, i)\n",
    "            if type == 'log_site':\n",
    "                self.log_sites.append(location)\n",
    "            elif type == 'sort_site':\n",
    "                self.sort_sites.append(location)\n",
    "            elif type == 'sawmill':\n",
    "                self.sawmill.append(location)\n",
    "            elif type == 'pulpmill':\n",
    "                self.pulpmill.append(location)\n",
    "\n",
    "        # define the action and observation space\n",
    "        # self.action_space = spaces.Discrete(len(self.available_actions))\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(self.size, self.size, 2), dtype=np.uint8)\n",
    "\n",
    "    #define the boundary function\n",
    "    def _check_boundary(self, x, y):\n",
    "        return -1 < x < self.size + 1 and -1 < y < self.size + 1\n",
    "    \n",
    "    # define the step function\n",
    "    def step(self, action):\n",
    "        # check if the action is valid\n",
    "        assert self.action_space.contains(action)\n",
    "        self.t += 1\n",
    "        self.reward = 0\n",
    "        reward = 60\n",
    "        # get the delta of the action\n",
    "        # get the delta of the action\n",
    "        for agent in self.agents:\n",
    "            # Sample a different action for each agent\n",
    "            action = self.action_space.sample()\n",
    "\n",
    "            # Get the delta of the action\n",
    "            delta = self.available_actions[action].delta\n",
    "\n",
    "            # Limit the delta to one position at a time\n",
    "            delta = (max(-1, min(1, delta[0])), max(-1, min(1, delta[1])))\n",
    "\n",
    "            # Move the agent\n",
    "            agent.move(delta, self.size)\n",
    "\n",
    "\n",
    "        # update the location volume\n",
    "        for i in range(len(self.log_sites)):\n",
    "            self.log_sites[i].logging()\n",
    "\n",
    "        for i in range(len(self.log_sites)):\n",
    "            self.log_sites[i].site_loading()\n",
    "        \n",
    "        # check if the agent is at the boundary\n",
    "        for agent in self.agents:\n",
    "            if not self._check_boundary(*agent.coordinates):\n",
    "                agent.reward += -10\n",
    "        \n",
    "        # location overload check and apply penalty to the environment\n",
    "        site_penalty = -2\n",
    "        for log_site in self.log_sites:\n",
    "            if log_site.volume > 100:\n",
    "                log_site.reward += site_penalty\n",
    "                # self.reward += log_site.reward\n",
    "        for sort_site in self.sort_sites:\n",
    "            if sort_site.volume > 100:\n",
    "                sort_site.reward += site_penalty\n",
    "                # self.reward += log_site.reward\n",
    "        for mill in self.sawmill:\n",
    "            if mill.volume > 100:\n",
    "                sort_site.reward += site_penalty\n",
    "                # self.reward += log_site.reward\n",
    "        for mill in self.pulpmill:\n",
    "            if mill.volume > 100:\n",
    "                sort_site.reward += site_penalty\n",
    "                # self.reward += log_site.reward\n",
    "\n",
    "\n",
    "        # check if the agent is at the log site, sort site, or mill, and make interactions\n",
    "        for agent in self.agents:\n",
    "            if agent.status == 0:  # Looking for log site\n",
    "                for i, log_site in enumerate(self.log_sites):\n",
    "                    if agent.coordinates == log_site.coordinates:\n",
    "                        agent.reward += reward\n",
    "                        agent.update_status(Status.LOADED_FROM_LOG_SITE)\n",
    "                        agent.logging_loading()\n",
    "                        self.log_sites[i].site_unloading()\n",
    "                        break\n",
    "\n",
    "            elif agent.status == 1:  # Looking for sort site\n",
    "                for i, sort_site in enumerate(self.sort_sites):\n",
    "                    if agent.coordinates == sort_site.coordinates:\n",
    "                        agent.reward += reward\n",
    "                        agent.update_status(Status.LOADED_SORTED_LOGS)\n",
    "                        agent.agent_unload()\n",
    "                        agent.agent_load()\n",
    "                        self.sort_sites[i].site_loading()\n",
    "                        break\n",
    "\n",
    "            elif agent.status == 2:  # Looking for mill\n",
    "                for i, mill in enumerate(self.sawmill):\n",
    "                    if agent.coordinates == mill.coordinates:\n",
    "                        agent.reward += reward\n",
    "                        agent.update_status(Status.UNLOADED_TO_MILL)\n",
    "                        agent.agent_unload()\n",
    "                        self.sawmill[i].site_loading()\n",
    "                        break\n",
    "                for i, mill in enumerate(self.pulpmill):\n",
    "                    if agent.coordinates == mill.coordinates:\n",
    "                        agent.reward += reward\n",
    "                        agent.update_status(Status.UNLOADED_TO_MILL)\n",
    "                        agent.agent_unload()\n",
    "                        self.pulpmill[i].site_loading()\n",
    "                        break\n",
    "\n",
    "            # check if the agent is done by the status and step exceed the limit \n",
    "            elif agent.status == 3:  # Agent status indicating termination\n",
    "                agent.terminate = True\n",
    "                agent.truncated = True\n",
    "\n",
    "        for agent in self.agents:\n",
    "            self.reward += agent.reward\n",
    "\n",
    "        # check if the agent is done\n",
    "        if all([agent.terminate for agent in self.agents]):\n",
    "            self.done = True\n",
    "        # check if the agent is truncated\n",
    "        if all([agent.truncated for agent in self.agents]):\n",
    "            self.truncated = True\n",
    "        # get the observation\n",
    "        obs = self._get_obs()\n",
    "        # get the info. the info includes the distance between the agent and the mill and the reward\n",
    "        info = {\"distance\": 0, \"reward\": self.reward}\n",
    "        for agent in self.agents:\n",
    "            for mill in self.sawmill:\n",
    "                info[\"distance\"] = np.linalg.norm(np.array(agent.coordinates) - np.array(mill.coordinates))\n",
    "                info[\"reward\"] = self.reward\n",
    "\n",
    "        return obs, self.reward, self.done, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs = np.zeros((self.size, self.size, 2), dtype=np.uint8)\n",
    "        for agent in self.agents:\n",
    "            if isinstance(agent.coordinates, tuple):  # Check if agent.coordinates is an instance of tuple\n",
    "                x, y = agent.coordinates\n",
    "                if 0 <= x < self.size and 0 <= y < self.size:\n",
    "                    obs[x, y, 0] = 1\n",
    "        return obs\n",
    "\n",
    "\n",
    "\n",
    "    # define the reset function\n",
    "    def reset(self):\n",
    "    # Create a list of (0, 0) tuples of the same length as the number of agents\n",
    "        initial_coordinates = [(0, 0) for _ in range(self.num_agents)]\n",
    "\n",
    "        # Create agents at the initial coordinates\n",
    "        self.agents = [Agent(index, coordinates, 0) for index, coordinates in enumerate(initial_coordinates)]\n",
    "\n",
    "        self.done = False\n",
    "        self.truncated = False\n",
    "        self.reward = 0\n",
    "        self.t = 0\n",
    "        return self._get_obs()\n",
    "\n",
    "    # define the render function\n",
    "    def render(self, mode=\"human\"):\n",
    "        if mode == \"human\":\n",
    "            # Create a grid of the same size as the environment\n",
    "            grid = np.zeros((self.size, self.size, 3), dtype=np.uint8)\n",
    "            # Color the grid based on the location of the agent\n",
    "            # for agent in self.agents:\n",
    "            #     if isinstance(agent.coordinates, tuple):\n",
    "            #         x, y = agent.coordinates\n",
    "            #         grid[x, y] = [0, 0, 0]  # Green color for agent   \n",
    "            # Color the grid based on the location of the log sites\n",
    "            for log_site in self.log_sites:\n",
    "                x, y = log_site.coordinates\n",
    "                grid[x, y] = [0, 0, 255]    # Blue color for log site\n",
    "            # Color the grid based on the location of the sort sites\n",
    "            for sort_site in self.sort_sites:\n",
    "                x, y = sort_site.coordinates\n",
    "                grid[x, y] = [255, 0, 0] # Red color for sort site\n",
    "            # Color the grid based on the location of the sawmill\n",
    "            for mill in self.sawmill:\n",
    "                x, y = mill.coordinates\n",
    "                grid[x, y] = [255, 255, 0]  # Yellow color for mill\n",
    "            # Color the grid based on the location of the pulpmill\n",
    "            for mill in self.pulpmill:\n",
    "                x, y = mill.coordinates\n",
    "                grid[x, y] = [255, 100, 0] # Orange color for mill\n",
    "             \n",
    "            # Display the grid\n",
    "            plt.imshow(grid)\n",
    "\n",
    "            for agent in self.agents:\n",
    "                if isinstance(agent.coordinates, tuple):\n",
    "                    x, y = agent.coordinates\n",
    "                    grid[x, y] = [0, 0, 0]  # Green color for agent  \n",
    "                    circle = patches.Circle((y, x), radius=0.5, color='green')  # Create a green circle at the agent's coordinates\n",
    "                    plt.gca().add_patch(circle)  # Add the circle to the plot\n",
    "                    plt.text(y, x, str(agent.volume), color='white', \n",
    "                            horizontalalignment='center', verticalalignment='center')\n",
    "                    \n",
    "            for log_site in self.log_sites:\n",
    "                x, y = log_site.coordinates\n",
    "                plt.text(y, x, str(log_site.volume), color='white', \n",
    "                        horizontalalignment='center', verticalalignment='center')\n",
    "                \n",
    "            for sort_site in self.sort_sites:   \n",
    "                x, y = sort_site.coordinates\n",
    "                plt.text(y, x, str(sort_site.volume), color='white', \n",
    "                        horizontalalignment='center', verticalalignment='center')\n",
    "                \n",
    "            for mill in self.sawmill:\n",
    "                x, y = mill.coordinates\n",
    "                plt.text(y, x, str(mill.volume), color='white', \n",
    "                        horizontalalignment='center', verticalalignment='center')\n",
    "                \n",
    "            for mill in self.pulpmill:\n",
    "                x, y = mill.coordinates\n",
    "                plt.text(y, x, str(mill.volume), color='white', \n",
    "                        horizontalalignment='center', verticalalignment='center')   \n",
    "\n",
    "\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        elif mode == \"rgb_array\":\n",
    "            obs = self._get_obs()\n",
    "            return obs\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "    # define the close function\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    # define the seed function\n",
    "    def seed(self, seed=None):\n",
    "        pass\n",
    "\n",
    "    # define the get action function\n",
    "    def get_action(self, action):\n",
    "        return self.available_actions[action]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jimmy\\.conda\\envs\\rl-gpu\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:25: UserWarning: \u001b[33mWARN: It seems a Box observation space is an image but the upper and lower bounds are not in [0, 255]. Generally, CNN policies assume observations are within that range, so you may encounter an issue if the observation values are not.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='ForestTruck-v0',\n",
    "    entry_point='__main__:ForestTruckEnv'  # Replace your_module_path with the actual path\n",
    ")\n",
    "\n",
    "import gym\n",
    "from __main__ import ForestTruckEnv\n",
    "\n",
    "env = gym.make('ForestTruck-v0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_observation = env.reset()\n",
    "# # print(\"Initial Observation:\", initial_observation)\n",
    "\n",
    "\n",
    "# for _ in range(100):  # Take 5 steps as an example\n",
    "#     action = env.action_space.sample()  # Randomly sample an action\n",
    "#     observation, reward, terminated, info = env.step(action)  # Take a step in the environment\n",
    "#     # print(f\"Action: {action}, Observation: {observation}, Reward: {reward}, Done: {terminated}, Info: {info}\")\n",
    "#     print(f\"Action: {action}, Reward: {reward}, Done: {terminated}, Info: {info}\")\n",
    "#     for i, agent in enumerate(env.agents):  # Assuming env.agents is a list of agents\n",
    "#         print(f'Agent {i} location: {agent.coordinates}, status: {agent.status}, volume: {agent.volume}')\n",
    "#     env.render()  # Render the environment\n",
    "#     if terminated:\n",
    "#         print(\"Episode finished after these many steps.\")\n",
    "#         break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jimmy\\.conda\\envs\\rl-gpu\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./tensorboard_logs/DQN_18\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 149      |\n",
      "|    ep_rew_mean      | 1.63e+04 |\n",
      "|    exploration_rate | 0.943    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 1705     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 595      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 106      |\n",
      "|    n_updates        | 123      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 221      |\n",
      "|    ep_rew_mean      | 2.5e+04  |\n",
      "|    exploration_rate | 0.832    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 1872     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1770     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 111      |\n",
      "|    n_updates        | 417      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 208      |\n",
      "|    ep_rew_mean      | 2.19e+04 |\n",
      "|    exploration_rate | 0.763    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 1851     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 2497     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 98.1     |\n",
      "|    n_updates        | 599      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 194      |\n",
      "|    ep_rew_mean      | 2.04e+04 |\n",
      "|    exploration_rate | 0.705    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 1843     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 3100     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 102      |\n",
      "|    n_updates        | 749      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 182      |\n",
      "|    ep_rew_mean      | 1.92e+04 |\n",
      "|    exploration_rate | 0.654    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 1814     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 3647     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 92.8     |\n",
      "|    n_updates        | 886      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 191      |\n",
      "|    ep_rew_mean      | 1.94e+04 |\n",
      "|    exploration_rate | 0.565    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 1776     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 4575     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 78.7     |\n",
      "|    n_updates        | 1118     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 195      |\n",
      "|    ep_rew_mean      | 2.02e+04 |\n",
      "|    exploration_rate | 0.482    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 1734     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 5457     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 69.6     |\n",
      "|    n_updates        | 1339     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 202      |\n",
      "|    ep_rew_mean      | 2.08e+04 |\n",
      "|    exploration_rate | 0.385    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 1689     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 6475     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 51.9     |\n",
      "|    n_updates        | 1593     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 196      |\n",
      "|    ep_rew_mean      | 1.96e+04 |\n",
      "|    exploration_rate | 0.329    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 1665     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 7067     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 39.4     |\n",
      "|    n_updates        | 1741     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 191      |\n",
      "|    ep_rew_mean      | 1.88e+04 |\n",
      "|    exploration_rate | 0.272    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 1642     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 7658     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 37       |\n",
      "|    n_updates        | 1889     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 204      |\n",
      "|    ep_rew_mean      | 1.93e+04 |\n",
      "|    exploration_rate | 0.147    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 1590     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 8977     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 19.5     |\n",
      "|    n_updates        | 2219     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 207      |\n",
      "|    ep_rew_mean      | 1.94e+04 |\n",
      "|    exploration_rate | 0.0567   |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 1556     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 9929     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 23.3     |\n",
      "|    n_updates        | 2457     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 207      |\n",
      "|    ep_rew_mean      | 1.94e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 1529     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 10762    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 70.3     |\n",
      "|    n_updates        | 2665     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 202      |\n",
      "|    ep_rew_mean      | 1.91e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 1513     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 11286    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 60.1     |\n",
      "|    n_updates        | 2796     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | 1.89e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 1494     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 11996    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 46.9     |\n",
      "|    n_updates        | 2973     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 198      |\n",
      "|    ep_rew_mean      | 1.88e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 1478     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 12644    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 33.8     |\n",
      "|    n_updates        | 3135     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | 1.91e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 1460     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 13578    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 16.8     |\n",
      "|    n_updates        | 3369     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 197      |\n",
      "|    ep_rew_mean      | 1.89e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 1451     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 14182    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 16.5     |\n",
      "|    n_updates        | 3520     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | 1.93e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 1438     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 15201    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 30.2     |\n",
      "|    n_updates        | 3775     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 199      |\n",
      "|    ep_rew_mean      | 1.94e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 1430     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 15928    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 34.9     |\n",
      "|    n_updates        | 3956     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 197      |\n",
      "|    ep_rew_mean      | 1.93e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 1423     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 16571    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 15.1     |\n",
      "|    n_updates        | 4117     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 193      |\n",
      "|    ep_rew_mean      | 1.88e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 1419     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 16963    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 21.1     |\n",
      "|    n_updates        | 4215     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 200      |\n",
      "|    ep_rew_mean      | 1.95e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 1407     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 18409    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 32.9     |\n",
      "|    n_updates        | 4577     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 207      |\n",
      "|    ep_rew_mean      | 2.03e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 1396     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 19905    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 38.9     |\n",
      "|    n_updates        | 4951     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 208      |\n",
      "|    ep_rew_mean      | 2.05e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 1391     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 20767    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 64.7     |\n",
      "|    n_updates        | 5166     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 207      |\n",
      "|    ep_rew_mean      | 2.04e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 1387     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 21320    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 40.7     |\n",
      "|    n_updates        | 5304     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 205      |\n",
      "|    ep_rew_mean      | 2.02e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 1382     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 22296    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 19       |\n",
      "|    n_updates        | 5548     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 211      |\n",
      "|    ep_rew_mean      | 2.09e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 1376     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 23627    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 22.6     |\n",
      "|    n_updates        | 5881     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 213      |\n",
      "|    ep_rew_mean      | 2.1e+04  |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 1373     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 24410    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.06     |\n",
      "|    n_updates        | 6077     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 213      |\n",
      "|    ep_rew_mean      | 2.1e+04  |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 1371     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 24955    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 27.7     |\n",
      "|    n_updates        | 6213     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 211      |\n",
      "|    ep_rew_mean      | 2.08e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 1368     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 25649    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 16.9     |\n",
      "|    n_updates        | 6387     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 216      |\n",
      "|    ep_rew_mean      | 2.14e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 1363     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 27065    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 24.1     |\n",
      "|    n_updates        | 6741     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 213      |\n",
      "|    ep_rew_mean      | 2.11e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 1360     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 27812    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 26.9     |\n",
      "|    n_updates        | 6927     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 218      |\n",
      "|    ep_rew_mean      | 2.14e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 1357     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 28828    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 25.9     |\n",
      "|    n_updates        | 7181     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 215      |\n",
      "|    ep_rew_mean      | 2.12e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 1356     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 29158    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 28.8     |\n",
      "|    n_updates        | 7264     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 210      |\n",
      "|    ep_rew_mean      | 2.11e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 1353     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 30020    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 101      |\n",
      "|    n_updates        | 7479     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 209      |\n",
      "|    ep_rew_mean      | 2.11e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 1351     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 30793    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 50.6     |\n",
      "|    n_updates        | 7673     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 218      |\n",
      "|    ep_rew_mean      | 2.24e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 1347     |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 32612    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 18.5     |\n",
      "|    n_updates        | 8127     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 223      |\n",
      "|    ep_rew_mean      | 2.28e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 1345     |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 33578    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 24.4     |\n",
      "|    n_updates        | 8369     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 222      |\n",
      "|    ep_rew_mean      | 2.27e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 1343     |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 34224    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 26.2     |\n",
      "|    n_updates        | 8530     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 226      |\n",
      "|    ep_rew_mean      | 2.31e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 1341     |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 35244    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 24.4     |\n",
      "|    n_updates        | 8785     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 220      |\n",
      "|    ep_rew_mean      | 2.25e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 1340     |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 35598    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 20.7     |\n",
      "|    n_updates        | 8874     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 223      |\n",
      "|    ep_rew_mean      | 2.27e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 1339     |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 36470    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 26.2     |\n",
      "|    n_updates        | 9092     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 224      |\n",
      "|    ep_rew_mean      | 2.27e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 1337     |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 37647    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 34.4     |\n",
      "|    n_updates        | 9386     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 225      |\n",
      "|    ep_rew_mean      | 2.26e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 1336     |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 38436    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 31.7     |\n",
      "|    n_updates        | 9583     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 234      |\n",
      "|    ep_rew_mean      | 2.32e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 1333     |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 39973    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 25.6     |\n",
      "|    n_updates        | 9968     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 244      |\n",
      "|    ep_rew_mean      | 2.42e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 1331     |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 41371    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 21.4     |\n",
      "|    n_updates        | 10317    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 236      |\n",
      "|    ep_rew_mean      | 2.35e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 1331     |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 41984    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 22.8     |\n",
      "|    n_updates        | 10470    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 226      |\n",
      "|    ep_rew_mean      | 2.24e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 1330     |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 42465    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 21.9     |\n",
      "|    n_updates        | 10591    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 224      |\n",
      "|    ep_rew_mean      | 2.2e+04  |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 1329     |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 43121    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 17.3     |\n",
      "|    n_updates        | 10755    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 228      |\n",
      "|    ep_rew_mean      | 2.22e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 1328     |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 44084    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 33.5     |\n",
      "|    n_updates        | 10995    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 235      |\n",
      "|    ep_rew_mean      | 2.3e+04  |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 1326     |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 45776    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 22.8     |\n",
      "|    n_updates        | 11418    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 239      |\n",
      "|    ep_rew_mean      | 2.3e+04  |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 1325     |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 47542    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 24.5     |\n",
      "|    n_updates        | 11860    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 235      |\n",
      "|    ep_rew_mean      | 2.27e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 1324     |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total_timesteps  | 47863    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 40.1     |\n",
      "|    n_updates        | 11940    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 242      |\n",
      "|    ep_rew_mean      | 2.27e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 1323     |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 49159    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 29.4     |\n",
      "|    n_updates        | 12264    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 243      |\n",
      "|    ep_rew_mean      | 2.29e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 1322     |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 49956    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 20.4     |\n",
      "|    n_updates        | 12463    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 234      |\n",
      "|    ep_rew_mean      | 2.19e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 1321     |\n",
      "|    time_elapsed     | 38       |\n",
      "|    total_timesteps  | 50470    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 59.5     |\n",
      "|    n_updates        | 12592    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 237      |\n",
      "|    ep_rew_mean      | 2.23e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 1320     |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 51548    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 47.3     |\n",
      "|    n_updates        | 12861    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 236      |\n",
      "|    ep_rew_mean      | 2.25e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 1320     |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 52387    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 34.4     |\n",
      "|    n_updates        | 13071    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 240      |\n",
      "|    ep_rew_mean      | 2.3e+04  |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 1319     |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 53185    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 15.2     |\n",
      "|    n_updates        | 13271    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 238      |\n",
      "|    ep_rew_mean      | 2.28e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 1318     |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 53855    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 31.8     |\n",
      "|    n_updates        | 13438    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 238      |\n",
      "|    ep_rew_mean      | 2.27e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 1318     |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 54605    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 35.6     |\n",
      "|    n_updates        | 13626    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 227      |\n",
      "|    ep_rew_mean      | 2.1e+04  |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 1318     |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total_timesteps  | 55275    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 44.1     |\n",
      "|    n_updates        | 13793    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 220      |\n",
      "|    ep_rew_mean      | 2.04e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 1317     |\n",
      "|    time_elapsed     | 42       |\n",
      "|    total_timesteps  | 55620    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 28.5     |\n",
      "|    n_updates        | 13879    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 226      |\n",
      "|    ep_rew_mean      | 2.09e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 1317     |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 56779    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 29.2     |\n",
      "|    n_updates        | 14169    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 219      |\n",
      "|    ep_rew_mean      | 2.02e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 1316     |\n",
      "|    time_elapsed     | 43       |\n",
      "|    total_timesteps  | 57167    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 40.7     |\n",
      "|    n_updates        | 14266    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 226      |\n",
      "|    ep_rew_mean      | 2.1e+04  |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 1315     |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 58214    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 38.2     |\n",
      "|    n_updates        | 14528    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 224      |\n",
      "|    ep_rew_mean      | 2.07e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 1315     |\n",
      "|    time_elapsed     | 44       |\n",
      "|    total_timesteps  | 58896    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 30       |\n",
      "|    n_updates        | 14698    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 217      |\n",
      "|    ep_rew_mean      | 2.01e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 1314     |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 59338    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 38.9     |\n",
      "|    n_updates        | 14809    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 216      |\n",
      "|    ep_rew_mean      | 2.02e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 1314     |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 60066    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 90       |\n",
      "|    n_updates        | 14991    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 207      |\n",
      "|    ep_rew_mean      | 1.95e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 1313     |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 60633    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 42.4     |\n",
      "|    n_updates        | 15133    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 201      |\n",
      "|    ep_rew_mean      | 1.9e+04  |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 1313     |\n",
      "|    time_elapsed     | 46       |\n",
      "|    total_timesteps  | 61447    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 32.1     |\n",
      "|    n_updates        | 15336    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 202      |\n",
      "|    ep_rew_mean      | 1.92e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 1312     |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total_timesteps  | 62192    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 27.5     |\n",
      "|    n_updates        | 15522    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 205      |\n",
      "|    ep_rew_mean      | 1.93e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 1312     |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total_timesteps  | 63001    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 35.6     |\n",
      "|    n_updates        | 15725    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 215      |\n",
      "|    ep_rew_mean      | 2e+04    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 1311     |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total_timesteps  | 64590    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 22.3     |\n",
      "|    n_updates        | 16122    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 215      |\n",
      "|    ep_rew_mean      | 2.02e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 1310     |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 65558    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 28.5     |\n",
      "|    n_updates        | 16364    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 202      |\n",
      "|    ep_rew_mean      | 1.88e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 1310     |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 65943    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 36.2     |\n",
      "|    n_updates        | 16460    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 190      |\n",
      "|    ep_rew_mean      | 1.8e+04  |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 1310     |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total_timesteps  | 66564    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 31.6     |\n",
      "|    n_updates        | 16615    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 195      |\n",
      "|    ep_rew_mean      | 1.82e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 1309     |\n",
      "|    time_elapsed     | 51       |\n",
      "|    total_timesteps  | 67393    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 33.6     |\n",
      "|    n_updates        | 16823    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 195      |\n",
      "|    ep_rew_mean      | 1.88e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 1309     |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total_timesteps  | 68658    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 25.6     |\n",
      "|    n_updates        | 17139    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 197      |\n",
      "|    ep_rew_mean      | 1.91e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 1308     |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 69644    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 59.2     |\n",
      "|    n_updates        | 17385    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 197      |\n",
      "|    ep_rew_mean      | 1.91e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 1308     |\n",
      "|    time_elapsed     | 53       |\n",
      "|    total_timesteps  | 70200    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 63.8     |\n",
      "|    n_updates        | 17524    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 203      |\n",
      "|    ep_rew_mean      | 1.96e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 1307     |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 71833    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 67.8     |\n",
      "|    n_updates        | 17933    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 199      |\n",
      "|    ep_rew_mean      | 1.93e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 1306     |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total_timesteps  | 72319    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 24.6     |\n",
      "|    n_updates        | 18054    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 201      |\n",
      "|    ep_rew_mean      | 1.92e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 1306     |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 73278    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 32.8     |\n",
      "|    n_updates        | 18294    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 205      |\n",
      "|    ep_rew_mean      | 1.97e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 1306     |\n",
      "|    time_elapsed     | 56       |\n",
      "|    total_timesteps  | 74346    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 28.2     |\n",
      "|    n_updates        | 18561    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 202      |\n",
      "|    ep_rew_mean      | 1.94e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 1305     |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 74797    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 52.3     |\n",
      "|    n_updates        | 18674    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 205      |\n",
      "|    ep_rew_mean      | 1.99e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 1305     |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 75776    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 44.8     |\n",
      "|    n_updates        | 18918    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 211      |\n",
      "|    ep_rew_mean      | 2.06e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 1305     |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 76671    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 45.9     |\n",
      "|    n_updates        | 19142    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 212      |\n",
      "|    ep_rew_mean      | 2.09e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 1304     |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total_timesteps  | 77994    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 24.8     |\n",
      "|    n_updates        | 19473    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 219      |\n",
      "|    ep_rew_mean      | 2.16e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 1304     |\n",
      "|    time_elapsed     | 60       |\n",
      "|    total_timesteps  | 79078    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 39.4     |\n",
      "|    n_updates        | 19744    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 215      |\n",
      "|    ep_rew_mean      | 2.11e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 1303     |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total_timesteps  | 79756    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 40.5     |\n",
      "|    n_updates        | 19913    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 223      |\n",
      "|    ep_rew_mean      | 2.21e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 1303     |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total_timesteps  | 81179    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 49       |\n",
      "|    n_updates        | 20269    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 228      |\n",
      "|    ep_rew_mean      | 2.24e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 1302     |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 82177    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 49.5     |\n",
      "|    n_updates        | 20519    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 229      |\n",
      "|    ep_rew_mean      | 2.25e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 1302     |\n",
      "|    time_elapsed     | 63       |\n",
      "|    total_timesteps  | 82961    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 38.7     |\n",
      "|    n_updates        | 20715    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 234      |\n",
      "|    ep_rew_mean      | 2.3e+04  |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 1302     |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 84053    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 30.3     |\n",
      "|    n_updates        | 20988    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 231      |\n",
      "|    ep_rew_mean      | 2.25e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 1302     |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total_timesteps  | 84546    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 38.1     |\n",
      "|    n_updates        | 21111    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 238      |\n",
      "|    ep_rew_mean      | 2.31e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 1301     |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 85998    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 48.5     |\n",
      "|    n_updates        | 21474    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 238      |\n",
      "|    ep_rew_mean      | 2.33e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 1301     |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 86768    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 34.8     |\n",
      "|    n_updates        | 21666    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 229      |\n",
      "|    ep_rew_mean      | 2.29e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 1301     |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 87484    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 40.3     |\n",
      "|    n_updates        | 21845    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 227      |\n",
      "|    ep_rew_mean      | 2.28e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 1301     |\n",
      "|    time_elapsed     | 67       |\n",
      "|    total_timesteps  | 88275    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 46.1     |\n",
      "|    n_updates        | 22043    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 231      |\n",
      "|    ep_rew_mean      | 2.32e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 1301     |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 89005    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 41.7     |\n",
      "|    n_updates        | 22226    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 232      |\n",
      "|    ep_rew_mean      | 2.32e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 1300     |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total_timesteps  | 89724    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 38.6     |\n",
      "|    n_updates        | 22405    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 235      |\n",
      "|    ep_rew_mean      | 2.39e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 1300     |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 90929    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 54.4     |\n",
      "|    n_updates        | 22707    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 230      |\n",
      "|    ep_rew_mean      | 2.35e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 1300     |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 91704    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 26.2     |\n",
      "|    n_updates        | 22900    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 226      |\n",
      "|    ep_rew_mean      | 2.3e+04  |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 1299     |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total_timesteps  | 92275    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 38.9     |\n",
      "|    n_updates        | 23043    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 229      |\n",
      "|    ep_rew_mean      | 2.3e+04  |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 428      |\n",
      "|    fps              | 1299     |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 93076    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 45.2     |\n",
      "|    n_updates        | 23243    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 219      |\n",
      "|    ep_rew_mean      | 2.19e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 432      |\n",
      "|    fps              | 1299     |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 93740    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 25.5     |\n",
      "|    n_updates        | 23409    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 219      |\n",
      "|    ep_rew_mean      | 2.19e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 436      |\n",
      "|    fps              | 1299     |\n",
      "|    time_elapsed     | 72       |\n",
      "|    total_timesteps  | 94210    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 40.8     |\n",
      "|    n_updates        | 23527    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 222      |\n",
      "|    ep_rew_mean      | 2.26e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 440      |\n",
      "|    fps              | 1299     |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total_timesteps  | 95478    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 26       |\n",
      "|    n_updates        | 23844    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 220      |\n",
      "|    ep_rew_mean      | 2.25e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 444      |\n",
      "|    fps              | 1299     |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 96394    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 25.9     |\n",
      "|    n_updates        | 24073    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 223      |\n",
      "|    ep_rew_mean      | 2.27e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 448      |\n",
      "|    fps              | 1299     |\n",
      "|    time_elapsed     | 74       |\n",
      "|    total_timesteps  | 97096    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 51.2     |\n",
      "|    n_updates        | 24248    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 225      |\n",
      "|    ep_rew_mean      | 2.26e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 452      |\n",
      "|    fps              | 1298     |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total_timesteps  | 98265    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 34.3     |\n",
      "|    n_updates        | 24541    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 227      |\n",
      "|    ep_rew_mean      | 2.27e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 456      |\n",
      "|    fps              | 1298     |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 99363    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 38.8     |\n",
      "|    n_updates        | 24815    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 219      |\n",
      "|    ep_rew_mean      | 2.19e+04 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 460      |\n",
      "|    fps              | 1298     |\n",
      "|    time_elapsed     | 76       |\n",
      "|    total_timesteps  | 99893    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 41.8     |\n",
      "|    n_updates        | 24948    |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "# Create the environment\n",
    "env = ForestTruckEnv(render_mode='human', size=8, reward=20, num_agents=1)\n",
    "\n",
    "# Instantiate the agent\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./tensorboard_logs/\")\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "\n",
    "# Save the trained agent\n",
    "model.save(\"dqn_mining_truck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The algorithm only supports (<class 'gymnasium.spaces.discrete.Discrete'>,) as action spaces but Discrete(4) was provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m env \u001b[38;5;241m=\u001b[39m Monitor(env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./tensorboard_logs/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Instantiate the agent with TensorBoard logging enabled\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDQN\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMlpPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./tensorboard_logs/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jimmy\\.conda\\envs\\rl-gpu\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:104\u001b[0m, in \u001b[0;36mDQN.__init__\u001b[1;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, target_update_interval, exploration_fraction, exploration_initial_eps, exploration_final_eps, max_grad_norm, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     78\u001b[0m     policy: Union[\u001b[38;5;28mstr\u001b[39m, Type[DQNPolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m     _init_setup_model: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    103\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# No action noise\u001b[39;49;00m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplay_buffer_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplay_buffer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstats_window_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats_window_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43msde_support\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimize_memory_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize_memory_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupported_action_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDiscrete\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupport_multi_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_initial_eps \u001b[38;5;241m=\u001b[39m exploration_initial_eps\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_final_eps \u001b[38;5;241m=\u001b[39m exploration_final_eps\n",
      "File \u001b[1;32mc:\\Users\\jimmy\\.conda\\envs\\rl-gpu\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:110\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.__init__\u001b[1;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, action_noise, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, use_sde_at_warmup, sde_support, supported_action_spaces)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     82\u001b[0m     policy: Union[\u001b[38;5;28mstr\u001b[39m, Type[BasePolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m     supported_action_spaces: Optional[Tuple[Type[spaces\u001b[38;5;241m.\u001b[39mSpace], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    109\u001b[0m ):\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstats_window_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats_window_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupport_multi_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msupport_multi_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmonitor_wrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_sde\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_sde\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43msde_sample_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msde_sample_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupported_action_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msupported_action_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size \u001b[38;5;241m=\u001b[39m buffer_size\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n",
      "File \u001b[1;32mc:\\Users\\jimmy\\.conda\\envs\\rl-gpu\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:180\u001b[0m, in \u001b[0;36mBaseAlgorithm.__init__\u001b[1;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vec_normalize_env \u001b[38;5;241m=\u001b[39m unwrap_vec_normalize(env)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m supported_action_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, supported_action_spaces), (\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe algorithm only supports \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_action_spaces\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as action spaces \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was provided\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m support_multi_env \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: the model does not support multiple envs; it requires \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma single vectorized environment.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: The algorithm only supports (<class 'gymnasium.spaces.discrete.Discrete'>,) as action spaces but Discrete(4) was provided"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Create the environment\n",
    "env = ForestTruckEnv(render_mode='human', size=8, reward=20, num_agents=1)\n",
    "\n",
    "# Wrap the environment with the Monitor wrapper\n",
    "env = Monitor(env, \"./tensorboard_logs/\")\n",
    "\n",
    "# Instantiate the agent with TensorBoard logging enabled\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./tensorboard_logs/\")\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "# Save the trained agent\n",
    "model.save(\"dqn_mining_truck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./tensorboard_logs/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadMonitorResultsError",
     "evalue": "No monitor files of the form *monitor.csv found in ./tensorboard_logs/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLoadMonitorResultsError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m results_plotter\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the results\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mresults_plotter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_results\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./tensorboard_logs/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Plot the results\u001b[39;00m\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\jimmy\\.conda\\envs\\rl-gpu\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:239\u001b[0m, in \u001b[0;36mload_results\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    237\u001b[0m monitor_files \u001b[38;5;241m=\u001b[39m get_monitor_files(path)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(monitor_files) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LoadMonitorResultsError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo monitor files of the form *\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMonitor\u001b[38;5;241m.\u001b[39mEXT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m data_frames, headers \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m monitor_files:\n",
      "\u001b[1;31mLoadMonitorResultsError\u001b[0m: No monitor files of the form *monitor.csv found in ./tensorboard_logs/"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common import results_plotter\n",
    "\n",
    "# Load the results\n",
    "results = results_plotter.load_results('./tensorboard_logs/')\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 5))\n",
    "results_plotter.plot_results(results, total_timesteps=100000, title='DQN Mining Truck')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
